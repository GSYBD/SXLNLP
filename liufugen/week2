"""
基于PyTorch框架，编写一个示例代码，实现机器学习部分任务。
规律： x 是一个 5维向量，如果该向量中所有元素之和属于[0,1)，则预测结果为1；
之和属于[1,2)，则预测结果为 2，以此类推。
基于交叉熵的多分类任务
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt


# 定义模型结构
class MyModel(nn.Module):
    def __init__(self, input_size=5):
        super(MyModel, self).__init__()
        # 定义神经网络
        self.fc1 = nn.Linear(input_size, 20)  # 线性层
        self.fc2 = nn.Linear(20,5)  # 线性层
        # # 定义激活函数 sigmoid 归一化函数
        # self.sigmoid = nn.Sigmoid()
        # 定义损失函数
        self.loss_fn = nn.CrossEntropyLoss()

    def forward(self, x, y=None):
        x = self.fc1(x)
        y_pred = self.fc2(x)
        if y is not None:
            # 计算损失
            return self.loss_fn(y_pred, y)
        else:
            return y_pred


def generate_sample():
    x = np.random.random(5)
    re_sum = np.sum(x)
    if re_sum < 1:
        return x, 0
    elif re_sum < 2:
        return x, 1
    elif re_sum < 3:
        return x, 2
    elif re_sum < 4:
        return x, 3
    else:
        return x, 4

# 生成训练数据
def generate_data(total_samples):
    X = []
    Y = []
    for i in range(total_samples):
        x, y = generate_sample()
        X.append(x)
        Y.append(y)
    return torch.FloatTensor(np.array(X)), torch.LongTensor(np.array(Y))


def test_model(model):
    # 定义测试数据
    test_data = generate_data(20)
    # 测试模型
    model.eval()
    # 获取测试数据
    x = test_data[0]
    y = test_data[1]
    # 预测结果
    correct, total = 0, 0
    with torch.no_grad():
        y_pred = model(x)  # 预测结果
        # print(y_pred)
        predicted = y_pred.argmax(dim=1)  # 获取预测类别
        correct += (predicted == y).sum().item()  # 正确预测的数量
        total += y.size(0)  # 总样本数量

    print(f'正确值：{y} \n 预测值：{predicted}')
    # 打印预测结果
    print(f'预测正确的数量: {correct}，预测总量：{total}, 预测准确率: {correct / total}')
    return correct/total


def train_model(model, x_data,y_data, num_epochs, batch_size, learning_rate):
    # 选择优化器
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    # 训练模型
    batch_loop = len(x_data) // batch_size
    log = {'loss': [], 'acc': []}
    for epoch in range(num_epochs):
        # 生成随机索引
        model.train()
        # 计算单批次的平均损失
        watch_loss = []
        # 遍历每个批次
        for i in range(batch_loop):
            # 获取当前批次索引
            x = x_data[i * batch_size:(i + 1) * batch_size]
            y = y_data[i * batch_size:(i + 1) * batch_size]
            # print(i, "\n", x, "\n", y)
            loss = model(x, y)
            # 计算梯度
            loss.backward()
            # 更新权重
            optimizer.step()
            # 清除梯度
            optimizer.zero_grad()
            watch_loss.append(loss.item())
            # break
        # 打印平均损失
        print(f'=======\n第{epoch + 1}平均loss: {np.mean(watch_loss)}')

        # 测试模型
        acc = test_model(model)
        log["loss"].append(np.mean(watch_loss))
        log["acc"].append(acc)
        if np.mean(watch_loss) < 0.01:
            break
    # 保存模型
    torch.save(model.state_dict(), 'my_model.pth')
    # 画图
    print(log)
    plt.plot(range(len(log["acc"])), log["acc"], label="acc") # 画acc曲线

    plt.plot(range(len(log["loss"])), log["loss"], label="loss")  # 画loss曲线
    plt.legend()
    plt.show()
    return


def create_train_task():
    # 创建模型实例
    model = MyModel(5)
    # 定义训练参数
    num_epochs = 100  # 训练轮数
    batch_size = 32  # 批次大小
    # 定义学习率
    learning_rate = 0.001
    # 生成训练数据
    x,y = generate_data(4096)

    # 训练模型
    train_model(model, x,y, num_epochs, batch_size, learning_rate)
    return


def predict_model(model_path,input_data):
    # 加载模型
    model = MyModel(5)
    model.load_state_dict(torch.load(model_path))
    # 预测结果
    model.eval()
    with torch.no_grad():
        y_pred = model(torch.FloatTensor(input_data))

    print(y_pred)
    print(y_pred.argmax(dim=1))
    return

if __name__ == '__main__':
    # print(generate_data(10))
    create_train_task()

    # generate_sample()
