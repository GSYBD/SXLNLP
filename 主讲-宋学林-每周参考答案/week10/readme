用bert实现自回归语言模型的训练，主要通过加入一个attention mask实现。
由前一个字预测下一个字，只要保证mask为下三角形状即可
