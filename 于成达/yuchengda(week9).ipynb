{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import jieba\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\"\"\"\n",
    "数据加载\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, data_path, config):\n",
    "        self.config = config\n",
    "        self.path = data_path\n",
    "        self.vocab = load_vocab(config[\"vocab_path\"])\n",
    "        self.config[\"vocab_size\"] = len(self.vocab)\n",
    "        self.sentences = []\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(config[\"bert_path\"])\n",
    "        self.schema = self.load_schema(config[\"schema_path\"])\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        self.data = []\n",
    "        with open(self.path, encoding=\"utf8\") as f:\n",
    "            segments = f.read().split(\"\\n\\n\")\n",
    "            for segment in segments:\n",
    "                sentenece = []\n",
    "                labels = []\n",
    "                for line in segment.split(\"\\n\"):\n",
    "                    if line.strip() == \"\":\n",
    "                        continue\n",
    "                    char, label = line.split()\n",
    "                    sentenece.append(char)\n",
    "                    labels.append(self.schema[label])\n",
    "                sentenece = \"\".join(sentenece)\n",
    "                self.sentences.append(\"\".join(sentenece))\n",
    "                input_ids = self.tokenizer.encode(sentenece, max_length=self.config[\"max_length\"],\n",
    "                                                  padding=\"max_length\", truncation=True)\n",
    "                # tokens = ['[CLS]'] + self.tokenizer.tokenize(\"\".join(sentenece)) + ['[SEP]']\n",
    "                # input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "                # input_ids = self.padding(input_ids, -1)\n",
    "                labels.insert(0, self.schema['O'])\n",
    "                # input_ids = self.encode_sentence(sentenece)\n",
    "                labels = self.padding(labels, -1)\n",
    "                self.data.append([torch.LongTensor(input_ids), torch.LongTensor(labels)])\n",
    "        return\n",
    "\n",
    "    def encode_sentence(self, text, padding=True):\n",
    "        input_id = []\n",
    "        if self.config[\"vocab_path\"] == \"words.txt\":\n",
    "            for word in jieba.cut(text):\n",
    "                input_id.append(self.vocab.get(word, self.vocab[\"[UNK]\"]))\n",
    "        else:\n",
    "            for char in text:\n",
    "                input_id.append(self.vocab.get(char, self.vocab[\"[UNK]\"]))\n",
    "        if padding:\n",
    "            input_id = self.padding(input_id)\n",
    "        return input_id\n",
    "\n",
    "    #补齐或截断输入的序列，使其可以在一个batch内运算\n",
    "    def padding(self, input_id, pad_token=0):\n",
    "        input_id = input_id[:self.config[\"max_length\"]]\n",
    "        input_id += [pad_token] * (self.config[\"max_length\"] - len(input_id))\n",
    "        return input_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def load_schema(self, path):\n",
    "        with open(path, encoding=\"utf8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "#加载字表或词表\n",
    "def load_vocab(vocab_path):\n",
    "    token_dict = {}\n",
    "    with open(vocab_path, encoding=\"utf8\") as f:\n",
    "        for index, line in enumerate(f):\n",
    "            token = line.strip()\n",
    "            token_dict[token] = index + 1  #0留给padding位置，所以从1开始\n",
    "    return token_dict\n",
    "\n",
    "#用torch自带的DataLoader类封装数据\n",
    "def load_data(data_path, config, shuffle=True):\n",
    "    dg = DataGenerator(data_path, config)\n",
    "    dl = DataLoader(dg, batch_size=config[\"batch_size\"], shuffle=shuffle)\n",
    "    return dl\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from config import Config\n",
    "    dg = DataGenerator(\"../ner_data/train.txt\", Config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from loader import load_data\n",
    "\n",
    "\"\"\"\n",
    "模型效果测试\n",
    "\"\"\"\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, config, model, logger):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.logger = logger\n",
    "        self.valid_data = load_data(config[\"valid_data_path\"], config, shuffle=False)\n",
    "\n",
    "\n",
    "    def eval(self, epoch):\n",
    "        self.logger.info(\"开始测试第%d轮模型效果：\" % epoch)\n",
    "        self.stats_dict = {\"LOCATION\": defaultdict(int),\n",
    "                           \"TIME\": defaultdict(int),\n",
    "                           \"PERSON\": defaultdict(int),\n",
    "                           \"ORGANIZATION\": defaultdict(int)}\n",
    "        self.model.eval()\n",
    "        for index, batch_data in enumerate(self.valid_data):\n",
    "            sentences = self.valid_data.dataset.sentences[index * self.config[\"batch_size\"]: (index+1) * self.config[\"batch_size\"]]\n",
    "            if torch.cuda.is_available():\n",
    "                batch_data = [d.cuda() for d in batch_data]\n",
    "            input_id, labels = batch_data   #输入变化时这里需要修改，比如多输入，多输出的情况\n",
    "            with torch.no_grad():\n",
    "                pred_results = self.model(input_id) #不输入labels，使用模型当前参数进行预测\n",
    "            self.write_stats(labels, pred_results, sentences)\n",
    "        self.show_stats()\n",
    "        return\n",
    "\n",
    "    def write_stats(self, labels, pred_results, sentences):\n",
    "        assert len(labels) == len(pred_results) == len(sentences)\n",
    "        if not self.config[\"use_crf\"]:\n",
    "            pred_results = torch.argmax(pred_results, dim=-1)\n",
    "        for true_label, pred_label, sentence in zip(labels, pred_results, sentences):\n",
    "            if not self.config[\"use_crf\"]:\n",
    "                pred_label = pred_label.cpu().detach().tolist()\n",
    "            true_label = true_label.cpu().detach().tolist()\n",
    "            true_entities = self.decode(sentence, true_label)\n",
    "            pred_entities = self.decode(sentence, pred_label)\n",
    "            print(\"=+++++++++\")\n",
    "            print(true_entities)\n",
    "            print(pred_entities)\n",
    "            print('=+++++++++')\n",
    "            # 正确率 = 识别出的正确实体数 / 识别出的实体数\n",
    "            # 召回率 = 识别出的正确实体数 / 样本的实体数\n",
    "            for key in [\"PERSON\", \"LOCATION\", \"TIME\", \"ORGANIZATION\"]:\n",
    "                self.stats_dict[key][\"正确识别\"] += len([ent for ent in pred_entities[key] if ent in true_entities[key]])\n",
    "                self.stats_dict[key][\"样本实体数\"] += len(true_entities[key])\n",
    "                self.stats_dict[key][\"识别出实体数\"] += len(pred_entities[key])\n",
    "        return\n",
    "\n",
    "    def show_stats(self):\n",
    "        F1_scores = []\n",
    "        for key in [\"PERSON\", \"LOCATION\", \"TIME\", \"ORGANIZATION\"]:\n",
    "            # 正确率 = 识别出的正确实体数 / 识别出的实体数\n",
    "            # 召回率 = 识别出的正确实体数 / 样本的实体数\n",
    "            precision = self.stats_dict[key][\"正确识别\"] / (1e-5 + self.stats_dict[key][\"识别出实体数\"])\n",
    "            recall = self.stats_dict[key][\"正确识别\"] / (1e-5 + self.stats_dict[key][\"样本实体数\"])\n",
    "            F1 = (2 * precision * recall) / (precision + recall + 1e-5)\n",
    "            F1_scores.append(F1)\n",
    "            self.logger.info(\"%s类实体，准确率：%f, 召回率: %f, F1: %f\" % (key, precision, recall, F1))\n",
    "        self.logger.info(\"Macro-F1: %f\" % np.mean(F1_scores))\n",
    "        correct_pred = sum([self.stats_dict[key][\"正确识别\"] for key in [\"PERSON\", \"LOCATION\", \"TIME\", \"ORGANIZATION\"]])\n",
    "        total_pred = sum([self.stats_dict[key][\"识别出实体数\"] for key in [\"PERSON\", \"LOCATION\", \"TIME\", \"ORGANIZATION\"]])\n",
    "        true_enti = sum([self.stats_dict[key][\"样本实体数\"] for key in [\"PERSON\", \"LOCATION\", \"TIME\", \"ORGANIZATION\"]])\n",
    "        micro_precision = correct_pred / (total_pred + 1e-5)\n",
    "        micro_recall = correct_pred / (true_enti + 1e-5)\n",
    "        micro_f1 = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall + 1e-5)\n",
    "        self.logger.info(\"Micro-F1 %f\" % micro_f1)\n",
    "        self.logger.info(\"--------------------\")\n",
    "        return\n",
    "\n",
    "    '''\n",
    "    {\n",
    "      \"B-LOCATION\": 0,\n",
    "      \"B-ORGANIZATION\": 1,\n",
    "      \"B-PERSON\": 2,\n",
    "      \"B-TIME\": 3,\n",
    "      \"I-LOCATION\": 4,\n",
    "      \"I-ORGANIZATION\": 5,\n",
    "      \"I-PERSON\": 6,\n",
    "      \"I-TIME\": 7,\n",
    "      \"O\": 8\n",
    "    }\n",
    "    '''\n",
    "    def decode(self, sentence, labels):\n",
    "        sentence = \"[[CLS]]\" + sentence\n",
    "        labels = \"\".join([str(x) for x in labels[:len(sentence) + 1]])\n",
    "        results = defaultdict(list)\n",
    "        for location in re.finditer(\"(04+)\", labels):\n",
    "            s, e = location.span()\n",
    "            results[\"LOCATION\"].append(sentence[s:e])\n",
    "        for location in re.finditer(\"(15+)\", labels):\n",
    "            s, e = location.span()\n",
    "            results[\"ORGANIZATION\"].append(sentence[s:e])\n",
    "        for location in re.finditer(\"(26+)\", labels):\n",
    "            s, e = location.span()\n",
    "            results[\"PERSON\"].append(sentence[s:e])\n",
    "        for location in re.finditer(\"(37+)\", labels):\n",
    "            s, e = location.span()\n",
    "            results[\"TIME\"].append(sentence[s:e])\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torchcrf import CRF\n",
    "from transformers import BertModel\n",
    "\"\"\"\n",
    "建立网络模型结构\n",
    "\"\"\"\n",
    "\n",
    "class TorchModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TorchModel, self).__init__()\n",
    "        hidden_size = config[\"hidden_size\"]\n",
    "        # vocab_size = config[\"vocab_size\"] + 1\n",
    "        max_length = config[\"max_length\"]\n",
    "        class_num = config[\"class_num\"]\n",
    "        num_layers = config[\"num_layers\"]\n",
    "        # self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "        # self.layer = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=True, num_layers=num_layers)\n",
    "        self.layer = BertModel.from_pretrained(config[\"bert_path\"], return_dict=False)\n",
    "        self.classify = nn.Linear(768, class_num)\n",
    "        self.crf_layer = CRF(class_num, batch_first=True)\n",
    "        self.use_crf = config[\"use_crf\"]\n",
    "        self.loss = torch.nn.CrossEntropyLoss(ignore_index=-1)  #loss采用交叉熵损失\n",
    "\n",
    "    #当输入真实标签，返回loss值；无真实标签，返回预测值\n",
    "    def forward(self, x, target=None):\n",
    "        # x = self.embedding(x)  #input shape:(batch_size, sen_len)\n",
    "        x, _ = self.layer(x)      #input shape:(batch_size, sen_len, input_dim)\n",
    "        predict = self.classify(x) #ouput:(batch_size, sen_len, num_tags) -> (batch_size * sen_len, num_tags)\n",
    "\n",
    "        if target is not None:\n",
    "            if self.use_crf:\n",
    "                mask = target.gt(-1) \n",
    "                return - self.crf_layer(predict, target, mask, reduction=\"mean\")\n",
    "            else:\n",
    "                #(number, class_num), (number)\n",
    "                return self.loss(predict.view(-1, predict.shape[-1]), target.view(-1))\n",
    "        else:\n",
    "            if self.use_crf:\n",
    "                return self.crf_layer.decode(predict)\n",
    "            else:\n",
    "                return predict\n",
    "\n",
    "\n",
    "def choose_optimizer(config, model):\n",
    "    optimizer = config[\"optimizer\"]\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    if optimizer == \"adam\":\n",
    "        return Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer == \"sgd\":\n",
    "        return SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from config import Config\n",
    "    model = TorchModel(Config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
