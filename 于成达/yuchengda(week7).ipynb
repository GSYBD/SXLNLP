{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train file and eval file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "with open(r\"E:\\badouai\\ai\\week7\\week7 文本分类问题\\文本分类练习.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)[1:]\n",
    "    data1 = defaultdict(list)\n",
    "    data_train = defaultdict(list)\n",
    "    data_eval = defaultdict(list)\n",
    "    for single_list in data:\n",
    "        data1[int(single_list[0])] += [single_list[1]]\n",
    "    for label, value in data1.items():\n",
    "        if label == 0:\n",
    "            data_eval[\"差评\"] = value[:100]\n",
    "            data_train[\"差评\"] = value[100:]\n",
    "        if label == 1:\n",
    "            data_eval[\"好评\"] = value[:100]\n",
    "            data_train[\"好评\"] = value[100:]\n",
    "with open(r\"E:\\badouai\\ai\\week7\\homework\\train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    first = True  # 用于跟踪是否是第一个字典项\n",
    "    for key, value in data_train.items():\n",
    "        for line in value:\n",
    "            # add_dict = {\"tag\": \"差评\" if key == 0 else \"好评\", \"title\": line}\n",
    "            if key == \"差评\":\n",
    "                add_dict = {\"tag\":\"差评\", \"title\": line}\n",
    "            elif key == \"好评\":\n",
    "                add_dict = {\"tag\":\"好评\", \"title\": line}\n",
    "            json_str = json.dumps(add_dict, ensure_ascii=False)  # 转换为 JSON 字符串\n",
    "            # 如果这不是第一个字典项，先写入一个换行符\n",
    "            if not first:\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                first = False\n",
    "            f.write(json_str)  # 写入 JSON 字符串\n",
    "\n",
    "with open(r\"E:\\badouai\\ai\\week7\\homework\\eval.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    first = True  # 用于跟踪是否是第一个字典项\n",
    "    for key, value in data_eval.items():\n",
    "        for line in value:\n",
    "            # add_dict = {\"tag\": \"差评\" if key == 0 else \"好评\", \"title\": line}\n",
    "            if key == \"差评\":\n",
    "                add_dict = {\"tag\":\"差评\", \"title\": line}\n",
    "            elif key == \"好评\":\n",
    "                add_dict = {\"tag\":\"好评\", \"title\": line}\n",
    "            json_str = json.dumps(add_dict, ensure_ascii=False)  # 转换为 JSON 字符串\n",
    "            # 如果这不是第一个字典项，先写入一个换行符\n",
    "            if not first:\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                first = False\n",
    "            f.write(json_str)  # 写入 JSON 字符串\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "配置参数信息\n",
    "\"\"\"\n",
    "\n",
    "Config = {\n",
    "    \"model_path\": r\"E:\\badouai\\ai\\week7\\homework\",\n",
    "    \"train_data_path\": r\"E:\\badouai\\ai\\week7\\homework\\train.json\",\n",
    "    \"valid_data_path\": r\"E:\\badouai\\ai\\week7\\homework\\eval.json\",\n",
    "    \"vocab_path\":r\"E:\\badouai\\ai\\week7\\week7 文本分类问题\\nn_pipline\\chars.txt\",\n",
    "    \"model_type\":\"bert\",\n",
    "    \"max_length\": 30,\n",
    "    \"hidden_size\": 256,\n",
    "    \"kernel_size\": 3,\n",
    "    \"num_layers\": 2,\n",
    "    \"epoch\": 15,\n",
    "    \"batch_size\": 128,\n",
    "    \"pooling_style\":\"max\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"pretrain_model_path\":r\"E:\\badouai\\ai\\week6\\bert-base-chinese\",\n",
    "    \"seed\": 987\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\"\"\"\n",
    "数据加载\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, data_path, config):\n",
    "        self.config = config\n",
    "        self.path = data_path\n",
    "        self.index_to_label = {0: '差评', 1: '好评'}\n",
    "        self.label_to_index = dict((y, x) for x, y in self.index_to_label.items())\n",
    "        self.config[\"class_num\"] = len(self.index_to_label)\n",
    "        if self.config[\"model_type\"] == \"bert\":\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(config[\"pretrain_model_path\"])\n",
    "        self.vocab = load_vocab(config[\"vocab_path\"])\n",
    "        self.config[\"vocab_size\"] = len(self.vocab)\n",
    "        self.load()\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        self.data = []\n",
    "        with open(self.path, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                line = json.loads(line)\n",
    "                tag = line[\"tag\"]\n",
    "                label = self.label_to_index[tag]\n",
    "                title = line[\"title\"]\n",
    "                if self.config[\"model_type\"] == \"bert\":\n",
    "                    input_id = self.tokenizer.encode(title, max_length=self.config[\"max_length\"], pad_to_max_length=True)\n",
    "                else:\n",
    "                    input_id = self.encode_sentence(title)\n",
    "                input_id = torch.LongTensor(input_id)\n",
    "                label_index = torch.LongTensor([label])\n",
    "                self.data.append([input_id, label_index])\n",
    "        return\n",
    "\n",
    "    def encode_sentence(self, text):\n",
    "        input_id = []\n",
    "        for char in text:\n",
    "            input_id.append(self.vocab.get(char, self.vocab[\"[UNK]\"]))\n",
    "        input_id = self.padding(input_id)\n",
    "        return input_id\n",
    "\n",
    "    #补齐或截断输入的序列，使其可以在一个batch内运算\n",
    "    def padding(self, input_id):\n",
    "        input_id = input_id[:self.config[\"max_length\"]]\n",
    "        input_id += [0] * (self.config[\"max_length\"] - len(input_id))\n",
    "        return input_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "def load_vocab(vocab_path):\n",
    "    token_dict = {}\n",
    "    with open(vocab_path, encoding=\"utf8\") as f:\n",
    "        for index, line in enumerate(f):\n",
    "            token = line.strip()\n",
    "            token_dict[token] = index + 1  #0留给padding位置，所以从1开始\n",
    "    return token_dict\n",
    "\n",
    "\n",
    "#用torch自带的DataLoader类封装数据\n",
    "def load_data(data_path, config, shuffle=True):\n",
    "    dg = DataGenerator(data_path, config)\n",
    "    dl = DataLoader(dg, batch_size=config[\"batch_size\"], shuffle=shuffle)\n",
    "    return dl\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from config import Config\n",
    "    dg = DataGenerator(\"valid_tag_news.json\", Config)\n",
    "    print(dg[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import logging\n",
    "from config import Config\n",
    "from model import TorchModel, choose_optimizer\n",
    "from evaluate import Evaluator\n",
    "from loader import load_data\n",
    "#[DEBUG, INFO, WARNING, ERROR, CRITICAL]\n",
    "logging.basicConfig(level=logging.INFO, format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\"\"\"\n",
    "模型训练主程序\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "seed = Config[\"seed\"] \n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def main(config):\n",
    "    #创建保存模型的目录\n",
    "    if not os.path.isdir(config[\"model_path\"]):\n",
    "        os.mkdir(config[\"model_path\"])\n",
    "    #加载训练数据\n",
    "    train_data = load_data(config[\"train_data_path\"], config)\n",
    "    #加载模型\n",
    "    model = TorchModel(config)\n",
    "    # 标识是否使用gpu\n",
    "    cuda_flag = torch.cuda.is_available()\n",
    "    if cuda_flag:\n",
    "        logger.info(\"gpu可以使用，迁移模型至gpu\")\n",
    "        model = model.cuda()\n",
    "    #加载优化器\n",
    "    optimizer = choose_optimizer(config, model)\n",
    "    #加载效果测试类\n",
    "    evaluator = Evaluator(config, model, logger)\n",
    "    #训练\n",
    "    for epoch in range(config[\"epoch\"]):\n",
    "        epoch += 1\n",
    "        model.train()\n",
    "        logger.info(\"epoch %d begin\" % epoch)\n",
    "        train_loss = []\n",
    "        for index, batch_data in enumerate(train_data):\n",
    "            if cuda_flag:\n",
    "                batch_data = [d.cuda() for d in batch_data]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, labels = batch_data   #输入变化时这里需要修改，比如多输入，多输出的情况\n",
    "            loss = model(input_ids, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            if index % int(len(train_data) / 2) == 0:\n",
    "                logger.info(\"batch loss %f\" % loss)\n",
    "        logger.info(\"epoch average loss: %f\" % np.mean(train_loss))\n",
    "        acc = evaluator.eval(epoch)\n",
    "    # model_path = os.path.join(config[\"model_path\"], \"epoch_%d.pth\" % epoch)\n",
    "    # torch.save(model.state_dict(), model_path)  #保存模型权重\n",
    "    return acc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main(Config)\n",
    "\n",
    "    # for model in [\"cnn\"]:\n",
    "    #     Config[\"model_type\"] = model\n",
    "    #     print(\"最后一轮准确率：\", main(Config), \"当前配置：\", Config[\"model_type\"])\n",
    "\n",
    "    #对比所有模型\n",
    "    #中间日志可以关掉，避免输出过多信息\n",
    "    # 超参数的网格搜索\n",
    "    with open(r\"E:\\badouai\\ai\\week7\\homework\\result.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        header = [\"model_type\", \"learning_rate\", \"hidden_size\", \"batch_size\", \"pooling_style\", \"acc\", \"time\"]\n",
    "        writer.writerow(header)\n",
    "        for model in [\"cnn\", \"rnn\", \"bert\", \"lstm\", \"gru\", \"gated_cnn\", \"rcnn\"]:\n",
    "            Config[\"model_type\"] = model\n",
    "            start_time = time.time()\n",
    "            print(f\"模型：{model}，开始时间：{start_time}\")\n",
    "            for lr in [1e-3, 1e-4]:\n",
    "                Config[\"learning_rate\"] = lr\n",
    "                if Config[\"model_type\"] == \"bert\":\n",
    "                    hidden_size_list = [768]\n",
    "                else:\n",
    "                    hidden_size_list = [128, 256]\n",
    "                for hidden_size in hidden_size_list:\n",
    "                    Config[\"hidden_size\"] = hidden_size\n",
    "                    for batch_size in [64, 128]:\n",
    "                        Config[\"batch_size\"] = batch_size\n",
    "                        for pooling_style in [\"avg\", \"max\"]:\n",
    "                            Config[\"pooling_style\"] = pooling_style\n",
    "                            print(\"最后一轮准确率：\", main(Config), \"当前配置：\", Config)\n",
    "                            writer.writerow([model, lr, hidden_size, batch_size, pooling_style, main(Config), time.time() - start_time])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_type\tlearning_rate\thidden_size\tbatch_size\tpooling_style\tacc\ttime\n",
    "                        \n",
    "cnn\t0.001\t128\t64\tavg\t0.855\t22.32502961\n",
    "                        \n",
    "cnn\t0.001\t128\t64\tmax\t0.86\t35.5318656\n",
    "                        \n",
    "cnn\t0.001\t128\t128\tavg\t0.85\t44.88531137\n",
    "                        \n",
    "cnn\t0.001\t128\t128\tmax\t0.865\t54.61551237\n",
    "                        \n",
    "cnn\t0.001\t256\t64\tavg\t0.845\t75.09645581\n",
    "                        \n",
    "cnn\t0.001\t256\t64\tmax\t0.845\t96.32786369\n",
    "                        \n",
    "cnn\t0.001\t256\t128\tavg\t0.855\t111.0196004\n",
    "                        \n",
    "cnn\t0.001\t256\t128\tmax\t0.845\t126.5093691\n",
    "                        \n",
    "cnn\t0.0001\t128\t64\tavg\t0.88\t139.3278177\n",
    "                        \n",
    "cnn\t0.0001\t128\t64\tmax\t0.87\t152.4910762\n",
    "                        \n",
    "cnn\t0.0001\t128\t128\tavg\t0.835\t161.7517056\n",
    "                        \n",
    "cnn\t0.0001\t128\t128\tmax\t0.875\t171.3785837\n",
    "                        \n",
    "cnn\t0.0001\t256\t64\tavg\t0.865\t191.924191\n",
    "                        \n",
    "cnn\t0.0001\t256\t64\tmax\t0.895\t213.1848226\n",
    "                        \n",
    "cnn\t0.0001\t256\t128\tavg\t0.85\t228.111635\n",
    "                        \n",
    "cnn\t0.0001\t256\t128\tmax\t0.875\t243.5794377\n",
    "                        \n",
    "rnn\t0.001\t128\t64\tavg\t0.86\t17.73826623\n",
    "                        \n",
    "rnn\t0.001\t128\t64\tmax\t0.87\t34.32513356\n",
    "                        \n",
    "rnn\t0.001\t128\t128\tavg\t0.865\t46.4845221\n",
    "                        \n",
    "rnn\t0.001\t128\t128\tmax\t0.875\t59.03364468\n",
    "                        \n",
    "rnn\t0.001\t256\t64\tavg\t0.86\t91.94913483\n",
    "                        \n",
    "rnn\t0.001\t256\t64\tmax\t0.855\t125.7109878\n",
    "                        \n",
    "rnn\t0.001\t256\t128\tavg\t0.85\t154.4917631\n",
    "                        \n",
    "rnn\t0.001\t256\t128\tmax\t0.865\t184.1074591\n",
    "                        \n",
    "rnn\t0.0001\t128\t64\tavg\t0.87\t200.4476593\n",
    "                        \n",
    "rnn\t0.0001\t128\t64\tmax\t0.875\t217.1574116\n",
    "                        \n",
    "rnn\t0.0001\t128\t128\tavg\t0.86\t229.2438855\n",
    "                        \n",
    "rnn\t0.0001\t128\t128\tmax\t0.885\t241.7149432\n",
    "                        \n",
    "rnn\t0.0001\t256\t64\tavg\t0.88\t274.6851103\n",
    "                        \n",
    "rnn\t0.0001\t256\t64\tmax\t0.865\t308.6718249\n",
    "                        \n",
    "rnn\t0.0001\t256\t128\tavg\t0.87\t337.4615014\n",
    "                        \n",
    "rnn\t0.0001\t256\t128\tmax\t0.87\t367.1906652\n",
    "                        \n",
    "bert\t0.001\t768\t64\tavg\t0.86\t335.9281619\n",
    "                        \n",
    "bert\t0.001\t768\t64\tmax\t0.865\t670.910861\n",
    "                        \n",
    "bert\t0.001\t768\t128\tavg\t0.86\t966.188699\n",
    "                        \n",
    "bert\t0.001\t768\t128\tmax\t0.865\t1260.317307\n",
    "                        \n",
    "bert\t0.0001\t768\t64\tavg\t0.87\t1595.422981\n",
    "                        \n",
    "bert\t0.0001\t768\t64\tmax\t0.89\t1929.996548\n",
    "                        \n",
    "bert\t0.0001\t768\t128\tavg\t0.875\t2224.901199\n",
    "                        \n",
    "bert\t0.0001\t768\t128\tmax\t0.89\t2519.612361\n",
    "                        \n",
    "lstm\t0.001\t128\t64\tavg\t0.89\t31.31020689\n",
    "                        \n",
    "lstm\t0.001\t128\t64\tmax\t0.875\t62.71541619\n",
    "                        \n",
    "lstm\t0.001\t128\t128\tavg\t0.865\t90.95092082\n",
    "                        \n",
    "lstm\t0.001\t128\t128\tmax\t0.885\t119.7199686\n",
    "                        \n",
    "lstm\t0.001\t256\t64\tavg\t0.875\t182.8570802\n",
    "                        \n",
    "lstm\t0.001\t256\t64\tmax\t0.875\t246.7106535\n",
    "                        \n",
    "lstm\t0.001\t256\t128\tavg\t0.88\t302.663357\n",
    "                        \n",
    "lstm\t0.001\t256\t128\tmax\t0.83\t359.1400239\n",
    "                        \n",
    "lstm\t0.0001\t128\t64\tavg\t0.885\t389.5645416\n",
    "                        \n",
    "lstm\t0.0001\t128\t64\tmax\t0.87\t420.3412535\n",
    "                        \n",
    "lstm\t0.0001\t128\t128\tavg\t0.88\t448.2398386\n",
    "                        \n",
    "lstm\t0.0001\t128\t128\tmax\t0.89\t476.4472737\n",
    "                        \n",
    "lstm\t0.0001\t256\t64\tavg\t0.85\t539.4177291\n",
    "                        \n",
    "lstm\t0.0001\t256\t64\tmax\t0.885\t603.303839\n",
    "                        \n",
    "lstm\t0.0001\t256\t128\tavg\t0.87\t659.3078237\n",
    "                        \n",
    "lstm\t0.0001\t256\t128\tmax\t0.9\t716.0215955\n",
    "                        \n",
    "gru\t0.001\t128\t64\tavg\t0.87\t23.07916331\n",
    "                        \n",
    "gru\t0.001\t128\t64\tmax\t0.875\t46.25065613\n",
    "                        \n",
    "gru\t0.001\t128\t128\tavg\t0.86\t66.63330102\n",
    "                        \n",
    "gru\t0.001\t128\t128\tmax\t0.9\t87.60907507\n",
    "                        \n",
    "gru\t0.001\t256\t64\tavg\t0.9\t142.9116254\n",
    "                        \n",
    "gru\t0.001\t256\t64\tmax\t0.875\t199.2451489\n",
    "                        \n",
    "gru\t0.001\t256\t128\tavg\t0.87\t246.8751929\n",
    "                        \n",
    "gru\t0.001\t256\t128\tmax\t0.875\t295.3396363\n",
    "                        \n",
    "gru\t0.0001\t128\t64\tavg\t0.885\t318.43646\n",
    "                        \n",
    "gru\t0.0001\t128\t64\tmax\t0.86\t341.7463377\n",
    "                        \n",
    "gru\t0.0001\t128\t128\tavg\t0.86\t361.912529\n",
    "                        \n",
    "gru\t0.0001\t128\t128\tmax\t0.88\t382.6267259\n",
    "                        \n",
    "gru\t0.0001\t256\t64\tavg\t0.9\t438.1744163\n",
    "                        \n",
    "gru\t0.0001\t256\t64\tmax\t0.88\t494.541379\n",
    "                        \n",
    "gru\t0.0001\t256\t128\tavg\t0.865\t542.051676\n",
    "                        \n",
    "gru\t0.0001\t256\t128\tmax\t0.87\t590.4650757\n",
    "                        \n",
    "gated_cnn\t0.001\t128\t64\tavg\t0.855\t15.45683718\n",
    "                        \n",
    "gated_cnn\t0.001\t128\t64\tmax\t0.865\t31.14057398\n",
    "                        \n",
    "gated_cnn\t0.001\t128\t128\tavg\t0.875\t43.1635766\n",
    "                        \n",
    "gated_cnn\t0.001\t128\t128\tmax\t0.87\t55.65457535\n",
    "                        \n",
    "gated_cnn\t0.001\t256\t64\tavg\t0.865\t87.34765458\n",
    "                        \n",
    "gated_cnn\t0.001\t256\t64\tmax\t0.88\t119.5271471\n",
    "                        \n",
    "gated_cnn\t0.001\t256\t128\tavg\t0.87\t142.6221111\n",
    "                        \n",
    "gated_cnn\t0.001\t256\t128\tmax\t0.855\t166.1181951\n",
    "                        \n",
    "gated_cnn\t0.0001\t128\t64\tavg\t0.885\t181.501816\n",
    "                        \n",
    "gated_cnn\t0.0001\t128\t64\tmax\t0.875\t196.9651101\n",
    "                        \n",
    "gated_cnn\t0.0001\t128\t128\tavg\t0.87\t209.1940687\n",
    "                        \n",
    "gated_cnn\t0.0001\t128\t128\tmax\t0.89\t221.6699221\n",
    "                        \n",
    "gated_cnn\t0.0001\t256\t64\tavg\t0.85\t253.3007152\n",
    "                        \n",
    "gated_cnn\t0.0001\t256\t64\tmax\t0.9\t285.5411942\n",
    "                        \n",
    "gated_cnn\t0.0001\t256\t128\tavg\t0.885\t308.5973182\n",
    "                        \n",
    "gated_cnn\t0.0001\t256\t128\tmax\t0.875\t332.1804705\n",
    "                        \n",
    "rcnn\t0.001\t128\t64\tavg\t0.84\t19.4196651\n",
    "                        \n",
    "rcnn\t0.001\t128\t64\tmax\t0.84\t40.13133216\n",
    "                        \n",
    "rcnn\t0.001\t128\t128\tavg\t0.865\t54.81967974\n",
    "                        \n",
    "rcnn\t0.001\t128\t128\tmax\t0.88\t69.92298007\n",
    "                        \n",
    "rcnn\t0.001\t256\t64\tavg\t0.86\t111.6694474\n",
    "                        \n",
    "rcnn\t0.001\t256\t64\tmax\t0.85\t153.8939385\n",
    "                        \n",
    "rcnn\t0.001\t256\t128\tavg\t0.855\t185.7154634\n",
    "                        \n",
    "rcnn\t0.001\t256\t128\tmax\t0.87\t217.971786\n",
    "                        \n",
    "rcnn\t0.0001\t128\t64\tavg\t0.87\t237.4363618\n",
    "                        \n",
    "rcnn\t0.0001\t128\t64\tmax\t0.89\t258.2464113\n",
    "                        \n",
    "rcnn\t0.0001\t128\t128\tavg\t0.875\t273.0485137\n",
    "                        \n",
    "rcnn\t0.0001\t128\t128\tmax\t0.875\t288.1557209\n",
    "                        \n",
    "rcnn\t0.0001\t256\t64\tavg\t0.86\t330.1321905\n",
    "                        \n",
    "rcnn\t0.0001\t256\t64\tmax\t0.865\t372.6673522\n",
    "                        \n",
    "rcnn\t0.0001\t256\t128\tavg\t0.875\t404.3369019\n",
    "                        \n",
    "rcnn\t0.0001\t256\t128\tmax\t0.89\t436.5389478\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
