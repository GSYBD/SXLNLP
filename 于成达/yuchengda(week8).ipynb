{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change in \"loader\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import jieba\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\"\"\"\n",
    "数据加载\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, data_path, config):\n",
    "        self.config = config\n",
    "        self.path = data_path\n",
    "        self.vocab = load_vocab(config[\"vocab_path\"])\n",
    "        self.config[\"vocab_size\"] = len(self.vocab)\n",
    "        self.schema = load_schema(config[\"schema_path\"])\n",
    "        self.train_data_size = config[\"epoch_data_size\"] #由于采取随机采样，所以需要设定一个采样数量，否则可以一直采\n",
    "        self.data_type = None  #用来标识加载的是训练集还是测试集 \"train\" or \"test\"\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        self.data = []\n",
    "        self.knwb = defaultdict(list)\n",
    "        with open(self.path, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                line = json.loads(line)\n",
    "                #加载训练集\n",
    "                if isinstance(line, dict):\n",
    "                    self.data_type = \"train\"\n",
    "                    questions = line[\"questions\"]\n",
    "                    label = line[\"target\"]\n",
    "                    for question in questions:\n",
    "                        input_id = self.encode_sentence(question)\n",
    "                        input_id = torch.LongTensor(input_id)\n",
    "                        self.knwb[self.schema[label]].append(input_id)\n",
    "                #加载测试集\n",
    "                else:\n",
    "                    self.data_type = \"test\"\n",
    "                    assert isinstance(line, list)\n",
    "                    question, label = line\n",
    "                    input_id = self.encode_sentence(question)\n",
    "                    input_id = torch.LongTensor(input_id)\n",
    "                    label_index = torch.LongTensor([self.schema[label]])\n",
    "                    self.data.append([input_id, label_index])\n",
    "        return\n",
    "\n",
    "    def encode_sentence(self, text):\n",
    "        input_id = []\n",
    "        if self.config[\"vocab_path\"] == \"words.txt\":\n",
    "            for word in jieba.cut(text):\n",
    "                input_id.append(self.vocab.get(word, self.vocab[\"[UNK]\"]))\n",
    "        else:\n",
    "            for char in text:\n",
    "                input_id.append(self.vocab.get(char, self.vocab[\"[UNK]\"]))\n",
    "        input_id = self.padding(input_id)\n",
    "        return input_id\n",
    "\n",
    "    #补齐或截断输入的序列，使其可以在一个batch内运算\n",
    "    def padding(self, input_id):\n",
    "        input_id = input_id[:self.config[\"max_length\"]]\n",
    "        input_id += [0] * (self.config[\"max_length\"] - len(input_id))\n",
    "        return input_id\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.data_type == \"train\":\n",
    "            return self.config[\"epoch_data_size\"]\n",
    "        else:\n",
    "            assert self.data_type == \"test\", self.data_type\n",
    "            return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.data_type == \"train\":\n",
    "            return self.random_train_sample() #随机生成一个训练样本\n",
    "        else:\n",
    "            return self.data[index]\n",
    "\n",
    "    #依照一定概率生成负样本或正样本\n",
    "    #负样本从随机两个不同的标准问题中各随机选取一个\n",
    "    #正样本从随机一个标准问题中随机选取两个\n",
    "    def random_train_sample(self):\n",
    "        standard_question_index = list(self.knwb.keys())\n",
    "        #随机正样本\n",
    "        if self.config[\"loss\"] == \"CosineEmbeddingLoss\":\n",
    "            if random.random() <= self.config[\"positive_sample_rate\"]:\n",
    "                p = random.choice(standard_question_index)\n",
    "                #如果选取到的标准问下不足两个问题，则无法选取，所以重新随机一次\n",
    "                if len(self.knwb[p]) < 2:\n",
    "                    return self.random_train_sample()\n",
    "                else:\n",
    "                    s1, s2 = random.sample(self.knwb[p], 2)\n",
    "                    return [s1, s2, torch.LongTensor([1])]\n",
    "            #随机负样本\n",
    "            else:\n",
    "                p, n = random.sample(standard_question_index, 2)\n",
    "                s1 = random.choice(self.knwb[p])\n",
    "                s2 = random.choice(self.knwb[n])\n",
    "                return [s1, s2, torch.LongTensor([-1])]\n",
    "        #########  产生两个正样本和一个负样本 #########\n",
    "        elif self.config[\"loss\"] == \"cosine_triplet_loss\":\n",
    "            p = random.choice(standard_question_index)\n",
    "            if len(self.knwb[p]) < 2:\n",
    "                return self.random_train_sample()\n",
    "            else:\n",
    "                s1, s2 = random.sample(self.knwb[p], 2)\n",
    "                p1 = random.choice(standard_question_index)\n",
    "                if p != p1 and len(self.knwb[p1]) > 0:\n",
    "                    s3 = random.choice(self.knwb[p1])\n",
    "                    return [s1, s2, s3]\n",
    "                else:\n",
    "                    return self.random_train_sample()\n",
    "\n",
    "\n",
    "\n",
    "#加载字表或词表\n",
    "def load_vocab(vocab_path):\n",
    "    token_dict = {}\n",
    "    with open(vocab_path, encoding=\"utf8\") as f:\n",
    "        for index, line in enumerate(f):\n",
    "            token = line.strip()\n",
    "            token_dict[token] = index + 1  #0留给padding位置，所以从1开始\n",
    "    return token_dict\n",
    "\n",
    "#加载schema\n",
    "def load_schema(schema_path):\n",
    "    with open(schema_path, encoding=\"utf8\") as f:\n",
    "        return json.loads(f.read())\n",
    "\n",
    "#用torch自带的DataLoader类封装数据\n",
    "def load_data(data_path, config, shuffle=True):\n",
    "    dg = DataGenerator(data_path, config)\n",
    "    dl = DataLoader(dg, batch_size=config[\"batch_size\"], shuffle=shuffle)\n",
    "    return dl\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from config import Config\n",
    "    dg = DataGenerator(\"valid_tag_news.json\", Config)\n",
    "    print(dg[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change in \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "from config import Config\n",
    "from model import SiameseNetwork, choose_optimizer\n",
    "from evaluate import Evaluator\n",
    "from loader import load_data\n",
    "\n",
    "logging.basicConfig(level = logging.INFO,format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\"\"\"\n",
    "模型训练主程序\n",
    "\"\"\"\n",
    "\n",
    "def main(config):\n",
    "    #创建保存模型的目录\n",
    "    if not os.path.isdir(config[\"model_path\"]):\n",
    "        os.mkdir(config[\"model_path\"])\n",
    "    #加载训练数据\n",
    "    train_data = load_data(config[\"train_data_path\"], config)\n",
    "    #加载模型\n",
    "    model = SiameseNetwork(config)\n",
    "    # 标识是否使用gpu\n",
    "    cuda_flag = torch.cuda.is_available()\n",
    "    if cuda_flag:\n",
    "        logger.info(\"gpu可以使用，迁移模型至gpu\")\n",
    "        model = model.cuda()\n",
    "    #加载优化器\n",
    "    optimizer = choose_optimizer(config, model)\n",
    "    #加载效果测试类\n",
    "    evaluator = Evaluator(config, model, logger)\n",
    "    #训练\n",
    "    for epoch in range(config[\"epoch\"]):\n",
    "        epoch += 1\n",
    "        model.train()\n",
    "        logger.info(\"epoch %d begin\" % epoch)\n",
    "        train_loss = []\n",
    "        for index, batch_data in enumerate(train_data):\n",
    "            optimizer.zero_grad()\n",
    "            if cuda_flag:\n",
    "                batch_data = [d.cuda() for d in batch_data]\n",
    "            if config[\"loss\"] == \"CosineEmbeddingLoss\":\n",
    "                input_id1, input_id2, labels = batch_data\n",
    "                loss = model(input_id1, input_id2, None, labels)\n",
    "            elif config[\"loss\"] == \"cosine_triplet_loss\":\n",
    "                input_id1, input_id2, input_id3 = batch_data\n",
    "                loss = model(input_id1, input_id2, input_id3, None)\n",
    "            else:\n",
    "                input_id1 = batch_data\n",
    "                loss = model(input_id1, None, None, None)\n",
    "            train_loss.append(loss.item())\n",
    "            # if index % int(len(train_data) / 2) == 0:\n",
    "            #     logger.info(\"batch loss %f\" % loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        logger.info(\"epoch average loss: %f\" % np.mean(train_loss))\n",
    "        evaluator.eval(epoch)\n",
    "    model_path = os.path.join(config[\"model_path\"], \"epoch_%d.pth\" % epoch)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(Config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change in \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\"\"\"\n",
    "建立网络模型结构\n",
    "\"\"\"\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SentenceEncoder, self).__init__()\n",
    "        hidden_size = config[\"hidden_size\"]\n",
    "        vocab_size = config[\"vocab_size\"] + 1\n",
    "        max_length = config[\"max_length\"]\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "        # self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    #输入为问题字符编码\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #使用lstm\n",
    "        # x, _ = self.lstm(x)\n",
    "        #使用线性层\n",
    "        x = self.layer(x)\n",
    "        x = nn.functional.max_pool1d(x.transpose(1, 2), x.shape[1]).squeeze()\n",
    "        return x\n",
    "\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.sentence_encoder = SentenceEncoder(config)\n",
    "        self.loss = nn.CosineEmbeddingLoss()\n",
    "\n",
    "    # 计算余弦距离  1-cos(a,b)\n",
    "    # cos=1时两个向量相同，余弦距离为0；cos=0时，两个向量正交，余弦距离为1\n",
    "    def cosine_distance(self, tensor1, tensor2):\n",
    "        tensor1 = torch.nn.functional.normalize(tensor1, dim=-1)\n",
    "        tensor2 = torch.nn.functional.normalize(tensor2, dim=-1)\n",
    "        cosine = torch.sum(torch.mul(tensor1, tensor2), axis=-1)\n",
    "        return 1 - cosine\n",
    "\n",
    "    def cosine_triplet_loss(self, a, p, n, margin=None):\n",
    "        ap = self.cosine_distance(a, p)\n",
    "        an = self.cosine_distance(a, n)\n",
    "        if margin is None:\n",
    "            diff = ap - an + 0.1\n",
    "        else:\n",
    "            diff = ap - an + margin.squeeze()\n",
    "        return torch.mean(diff[diff.gt(0)]) #greater than\n",
    "\n",
    "####### 在之前代码的基础上重新添加了cosine_triplet_loss函数\n",
    "    def forward(self, sentence1, sentence2=None, sentence3=None, target=None):\n",
    "        #同时传入两个句子\n",
    "        if sentence2 is not None:\n",
    "            vector1 = self.sentence_encoder(sentence1) #vec:(batch_size, hidden_size)\n",
    "            vector2 = self.sentence_encoder(sentence2)\n",
    "            if sentence3 is not None:\n",
    "                vector3 = self.sentence_encoder(sentence3)\n",
    "                return self.cosine_triplet_loss(vector1, vector2, vector3)\n",
    "            #如果有标签，则计算loss\n",
    "            if target is not None:\n",
    "                return self.loss(vector1, vector2, target.squeeze())\n",
    "            #如果无标签，计算余弦距离\n",
    "            else:\n",
    "                return self.cosine_distance(vector1, vector2)\n",
    "        #单独传入一个句子时，认为正在使用向量化能力\n",
    "        else:\n",
    "            return self.sentence_encoder(sentence1)\n",
    "\n",
    "\n",
    "def choose_optimizer(config, model):\n",
    "    optimizer = config[\"optimizer\"]\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    if optimizer == \"adam\":\n",
    "        return Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer == \"sgd\":\n",
    "        return SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from config import Config\n",
    "    Config[\"vocab_size\"] = 10\n",
    "    Config[\"max_length\"] = 4\n",
    "    model = SiameseNetwork(Config)\n",
    "    s1 = torch.LongTensor([[1,2,3,0], [2,2,0,0]])\n",
    "    s2 = torch.LongTensor([[1,2,3,4], [3,2,3,4]])\n",
    "    l = torch.LongTensor([[1],[0]])\n",
    "    y = model(s1, s2, l)\n",
    "    print(y)\n",
    "    # print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change in \"config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 多增加了以下配置\n",
    "\"loss\": \"cosine_triplet_loss\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "badouai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
