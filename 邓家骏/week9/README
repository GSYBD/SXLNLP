目标：
当前ner实现是embedding + lstm ,现在改用bert实现

1.先理解一波crf? 
    A:
    四步：
    1. 输入序列X，输出序列为Y的分数
        X指的是样本，经过模型后得到发射矩阵，sentence_len * label
        Y指的是Y_true,带入公式后可理解为，按正确路径走，取预测路径+发射矩阵路径的得分
    2. 输入序列X，预测输出序列为Y的概率
        y~指的是，所有可能的路径，取最大值就是让预测路径更贴近正确路径。
    3. 取log，最大化值
        如上
    4. 取相反数做loss，最小化值

2. 我感觉如果用bert，crf其实是起到加强，只针对实体（正样本？）的注意力。感觉会更容易出现过拟合的效果。


addition:
sentence_level_sequence_labeling任务  未开始

记录：
    1.loader.py
    bert的中文分词：
        按字分词
    