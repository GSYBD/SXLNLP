inputX = ['举例说明', '两句话']
'''
 过embedding层
 假设词典有3000字, 模型的隐藏层设置是768
 token_embedding层的形状就是3000 * 768
 segment_embedding层的形状是2 * 768
 postiton_embedding层的形状是10 * 768 (七个字加一个句首符两个句尾符)
 embedding层的训练数是3000 * 768 + 2 * 768 + 10 * 768
 '''
'''
 之后过self-attention
 三个线性层层参数都是 768 * 10 三个b都是 10 * 768
 这一层训练数有 (768 * 10 + 10 * 768) * 3
 出来后还要过一个 768 * 768 的线性层
 一个w一个b, b应该还是 10 * 768, 训练数是 768 * 768 + 10 * 768
'''
'''
残差没有加入新的权重
残差后过归一化层, 因为出来形状没变应该是和前面的线性层一样
训练数是 768 * 768 + 10 * 768
'''
'''
Feed Forward层
里面的线性层w是 768 * (4 * 768) b应该是 10 * (4 * 768)
激活层没有添加新的权重
外面的线性层w是 (4 * 768) * 768 b 应该是 10 * 768
这一层的训练数是 768 * (4 * 768) + 10 * (4 * 768) + (4 * 768) * 768 + 10 * 768
'''
'''
残差没有加入新的权重
残差后过归一化层, 因为出来形状没变应该是和前面的线性层一样
训练数是 768 * 768 + 10 * 768
'''
# 总训练数
num = 3000 * 768 + 2 * 768 + 10 * 768 + (768 * 10 + 10 * 768) * 3 + 768 * 768 + 10 * 768 + 768 * 768 + 10 * 768 + 768 * (4 * 768) + 10 * (4 * 768) + (4 * 768) * 768 + 10 * 768 + 768 * 768 + 10 * 768
# 这是一层的, 一共12层最后还要承12
result = num * 12  # 106905600
print(result)
