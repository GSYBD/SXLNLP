import torch.nn as nn
import torch
import numpy as np
import random
import matplotlib.pyplot as plt

"""
使用RNN进行文本分类
一个包含 甲乙丙丁戊己庚辛壬癸 的 8 个长度字符。
0：甲的 index 为 0,1
1：甲的 index 为 2,3
2：甲的 index 为 4,5
3：甲的 index 为 6,7
4：甲不存在
"""
sentence_length, vocab_map, vocab = 8, {}, []


class TorchModel(nn.Module):
    def __init__(self, output_size, vocab_map, embding_dim=5):
        """
        初始化模型
        :param sentence_length: 句子长度
        :param output_size: 输出大小
        :param vocab_map: 词汇表
        :param embding_dim: 嵌入维度
        """
        super(TorchModel, self).__init__()
        hidden_size = 10
        self.embedding = nn.Embedding(len(vocab_map), embding_dim, padding_idx=0)
        # self.pooling = nn.AvgPool1d(sentence_length)
        self.run = nn.RNN(embding_dim, hidden_size, batch_first=True, bias=True)
        self.activation = nn.Sigmoid()  # 归一化函数
        # self.fc = nn.Linear(embding_dim, output_size, bias=False)
        self.fc = nn.Linear(hidden_size, output_size, bias=False)
        self.loos = nn.functional.cross_entropy  # 损失函数

    def forward(self, x, y=None):
        """
        前向传播
        :param x: 输入
        :param y: 标签
        :return: 预测值
        """
        x = self.embedding(x)
        # x = x.transpose(1, 2)  # 交换维度，由于 pooling 根据最后一维进行池化
        # x = self.pooling(x)
        # x = x.squeeze()
        _, x = self.run(x)
        x = x.squeeze(0)
        x = self.activation(x)
        y_pred = self.fc(x)
        if y is not None:
            # print(y_pred, "\n", y)
            # print(f'y_pred.shape:{y_pred.shape}, y.len:{len(y)}')
            return self.loos(y_pred, y)
        else:
            return y_pred


def build_matrix():
    """
    构建词嵌入矩阵
    :return: 词表矩阵,词汇表
    """
    chars = "甲乙丙丁戊己庚辛壬癸"  # 字符集
    vocab_map = {word: i + 1 for i, word in enumerate(chars)}
    vocab_map["pad"] = 0
    vocab_map['unk'] = len(vocab_map)  # 26
    return vocab_map, list(vocab_map.keys())


def build_smaple(vocab_map, sentence):
    """
    构建样本
    :param vocab_map: 词汇表映射
    :param sentence: 数组
    :return: 样本
    """
    x = [vocab_map.get(word, vocab_map["unk"]) for word in sentence]
    # 构建样本
    if '甲' in sentence:
        index = sentence.index('甲')
        y = index // 2
    else:
        y = 4
    return x, y


def build_batch(vocab_map, vocab, all_size, sentence_length=7):
    """
    构建批次
    :param vocab_map: 词汇表映射
    :param vocab: 词汇表
    :param batch_size: 批次大小
    :param sentence_length: 句子长度
    :return: 批次
    """
    # 构建批次
    data_x, data_y = [], []
    for _ in range(all_size):
        sentence = [random.choice(vocab) for _ in range(sentence_length)]
        x, y = build_smaple(vocab_map, sentence)
        # print(sentence,"    ",x, "  ",y)
        data_x.append(x)
        data_y.append(y)

    data_x = torch.LongTensor(data_x)
    data_y = torch.LongTensor(data_y)
    # print(data_x, data_y)
    return data_x, data_y


def test_model(model):
    data_x, data_y = build_batch(vocab_map, vocab, 200, sentence_length)

    model.eval()
    with torch.no_grad():
        data_ypred = model(data_x)
        y_pred = torch.argmax(data_ypred, dim=1)
        correct = (y_pred == data_y).sum().item()
        total = len(data_y)

    print(f'预测正确的数量: {correct}, 总量: {total}， 预测准确率: {correct / total}')
    # print(y_pred, "\n", data_y)
    return correct / total


def train_model(model, batch_size, epoch_num, learning_rate=0.001):
    """
    训练模型
    :param model: 模型
    :param data_x: 输入
    :param data_y: 标签
    :param batch_size: 批次大小
    :param epoch: 迭代次数
    :return: 模型
    """
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    log = {'loss': [], 'accuracy': []}

    for epoch in range(epoch_num):
        model.train()
        watch_loss = []
        # for i in range(0, len(data_x), batch_size):
        #     x = data_x[i:i + batch_size]
        #     y = data_y[i:i + batch_size]
        for batch in range(int(5012 / batch_size)):
            x, y = build_batch(vocab_map, vocab, batch_size, sentence_length)  # 构造一组训练样本
            optimizer.zero_grad()
            loss = model(x, y)
            loss.backward()
            optimizer.step()
            watch_loss.append(loss.item())
        # 求平均
        mean_loss = np.mean(watch_loss)
        print(f'=======\n第{epoch + 1}平均loss: {mean_loss}')

        # 测试，记录
        acc = test_model(model)
        log['loss'].append(mean_loss)
        log['accuracy'].append(acc)

    torch.save(model.state_dict(), "nlp_model2.pth")

    print(log)
    plt.plot(range(len(log['loss'])), log['loss'], label='loss')
    plt.plot(range(len(log['accuracy'])), log['accuracy'], label='accuracy')
    plt.legend()
    plt.show()


def create_train_task():
    output_size = 5
    batch_size = 16
    epoch_num = 50

    global vocab_map, vocab
    vocab_map, vocab = build_matrix()

    # data_x, data_y = build_batch(vocab_map, vocab, all_size, sentence_length)
    torch_model = TorchModel(sentence_length, output_size, vocab_map, embding_dim=30)
    train_model(torch_model, batch_size, epoch_num)


def predict_model(model_path, data_x, data_y):
    output_size = 5

    model = TorchModel(output_size, vocab_map,30)
    model.load_state_dict(torch.load(model_path))

    model.eval()
    with torch.no_grad():
        data_ypred = model(data_x)
        y_pred = torch.argmax(data_ypred, dim=1)
        correct = (y_pred == data_y).sum().item()
        total = len(data_y)

    print(f'预测正确的数量: {correct}, 总量: {total}， 预测准确率: {correct / total}')
    print(y_pred, "\n", data_y)
    return


if __name__ == '__main__':
    vocab_map, vocab = build_matrix()
    data_x,data_y = build_batch(vocab_map, vocab, 10, sentence_length)
    predict_model("nlp_model2.pth",data_x,data_y)
    # create_train_task()
