

# embedding
words_embeddings_weight = 21128 * 768
position_embeddings_weight = 512 * 768  
token_type_embeddings_weight = 2 * 768
layer_norm_weight = 768
layer_norm_bias = 768

# Q,K,V   è¾“å…¥x = sentence_length x hidden_size(768)
query_weight = 768 * 768
query_bias = 768
key_weight = 768 * 768
key_bias = 768
value_weight = 768 * 768
value_bias = 768

# ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡=ğ¿ğ‘–ğ‘›ğ‘’ağ‘Ÿ(ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„,ğ¾,ğ‘‰))  ç»è¿‡çº¿æ€§å±‚
attention_output_dense_weight = 768 * 768
attention_output_dense_bias = 768

# LayerNorm(Xembedding+ Xattention)
attention_layer_norm_weight = 768
attention_layer_norm_bias = 768

# ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡=ğ¿ğ‘–ğ‘›ğ‘’ağ‘Ÿ(ğ‘”ğ‘’ğ‘™ğ‘¢(ğ¿ğ‘–ğ‘›ğ‘’ağ‘Ÿ(ğ‘¥)))
intermediate_dense_weight = 768 * (768 * 4)
intermediate_dense_bias = 768 * 4
output_dense_weight = (768 * 4) * 768
output_dense_bias = 768

# LayerNorm(X forward+ X attention)
output_layer_norm_weight = 768
output_layer_norm_bias = 768

# BERTæ¨¡å‹åœ¨ç»è¿‡å¤šä¸ªç¼–ç å±‚ä¹‹åï¼Œä¼šç»è¿‡ä¸€ä¸ªç‰¹æ®Šçš„Poolerå±‚ï¼Œç”¨äºä»åºåˆ—ä¸­æå–ä¸€ä¸ªå›ºå®šé•¿åº¦çš„ç‰¹å¾å‘é‡ã€‚è¿™ä¸ªç‰¹å¾å‘é‡å¯ä»¥ç”¨äºåˆ†ç±»ä»»åŠ¡æˆ–å…¶ä»–ä»»åŠ¡ã€‚
# åœ¨BERTæ¨¡å‹ä¸­ï¼ŒPoolerå±‚é€šå¸¸æ˜¯ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ï¼Œå®ƒæ¥å—ç¼–ç å±‚çš„è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªå›ºå®šé•¿åº¦çš„ç‰¹å¾å‘é‡ã€‚
pooler_dense_weight = 768 * 768
pooler_dense_bias = 768


