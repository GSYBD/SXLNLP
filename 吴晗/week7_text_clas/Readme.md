BERT系列的测了一下,训练起来太慢了,没时间跑了,后续会补上,已经经过了测试,代码层面是可以跑的
另外再用BERT时候发现一个很神奇的事
在loader.py的第23行,设置标签对应编号那里,如果把标签设置为1和2,会直接CUDA报错,但是如果设置为0和1的话,0会对应到BERT Tokenizer的<pad>, 因为长度padding也是用这个符号,会对预测有影响吗?
